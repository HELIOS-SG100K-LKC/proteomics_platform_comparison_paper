{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../../data/Thermo/Preprocessed _data/Thermo_Fisher_Merged_All_Imputed.csv\")\n",
    "#df = pd.read_csv(\"../data/illumina/Preprocessed_data/Illumina_Merged_Unique.csv\")\n",
    "# df = pd.read_csv(\"./data/Olink_Merged_Unique.csv\")\n",
    "pheno = pd.read_csv(\"../data/HELIOS_Core_v4.csv\")\n",
    "\n",
    "# Filter unique participants\n",
    "df_unique = df[(df['tech_rep_id'] == \"N\") & (df['bio_rep_id'] == \"N\") | (df['tech_rep'] == 1)]\n",
    "\n",
    "display(df_unique.head())\n",
    "\n",
    "# number of samples\n",
    "print(\"Number of unique samples:\", df_unique.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset and factorize phenotypes\n",
    "pheno = (pheno\n",
    "         .assign(Bmi=lambda x: 100*100*x['DBI14_Weight'] / (x['DBI13_Height']**2))\n",
    "         .assign(Age=lambda x: x['FREG8_Age'],\n",
    "                 Sex=lambda x: np.where(x['FREG7_Gender'] == \"F\", 1, 0))\n",
    "         .loc[:, ['FREG1_Barcode', 'Age', 'Sex', 'FREG5_Ethnic_Group', 'Bmi']]\n",
    "         .query('FREG5_Ethnic_Group != \"O\"'))\n",
    "\n",
    "\n",
    "pheno['Sex'] = pheno['Sex'].astype('category')\n",
    "\n",
    "# Convert 'FREG5_Ethnic_Group' to categorical dummy variables\n",
    "pheno = pd.get_dummies(pheno, columns=['FREG5_Ethnic_Group'], drop_first=True)\n",
    "print(pheno.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with proteomics data\n",
    "df_unique = pd.merge(pheno, df_unique, on='FREG1_Barcode')\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Separate into phenotype and proteomic datasets\n",
    "df_pheno = df_unique.iloc[:, :14]\n",
    "display(df_pheno.head())\n",
    "df_proteome = df_unique.iloc[:, 14:]\n",
    "display(df_proteome.head())\n",
    "\n",
    "# Remove proteins with only one value\n",
    "df_proteome = df_proteome.loc[:, df_proteome.apply(lambda x: x.nunique()) != 1]\n",
    "\n",
    "print(df_proteome.shape)\n",
    "\n",
    "df_proteome_median = df_proteome.median()\n",
    "#set first column as index of the dataframe\n",
    "df_proteome_median = df_proteome_median.reset_index()\n",
    "df_proteome_median.columns = ['Protein', 'Median']\n",
    "#display(df_proteome_median.head())\n",
    "df_proteome_median.set_index('Protein', inplace=True)\n",
    "#display(df_proteome_median.head())\n",
    "\n",
    "# Function to apply inverse normal transformation\n",
    "def inverse_normal_transform(series):\n",
    "    ranks = stats.rankdata(series)  # Rank the data\n",
    "    ranks = (ranks - 0.5) / len(series)  # Convert ranks to percentiles\n",
    "    transformed = stats.norm.ppf(ranks)  # Apply inverse normal transformation\n",
    "    return transformed\n",
    "\n",
    "# Do rank-based inverse normal transformation of the phenotypes Age and BMI to make them normally distributed\n",
    "df_pheno['Age'] = inverse_normal_transform(df_pheno['Age'])\n",
    "df_pheno['Bmi'] = inverse_normal_transform(df_pheno['Bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the inforamtion on different platform versions\n",
    "def return_protein_list():\n",
    "    #read xlsx file\n",
    "    df1 = pd.read_excel('../../platform_analytes_list/somalogic_5k.xlsx', header=1)\n",
    "    #display(df1.head())\n",
    "    uniprot_list_v1 = df1['UniProt'].tolist()\n",
    "    sequence_list_v1 = df1['SeqId'].tolist()\n",
    "\n",
    "    df2 = pd.read_excel('../../platform_analytes_list/somalogic_7k.xlsx', header=1)\n",
    "    #display(df2.head())\n",
    "    uniprot_list_v2 = df2['UniProt'].tolist()\n",
    "    sequence_list_v2 = df2['SeqId'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "    df3 = pd.read_csv('../../data/somalogic/Preprocessed_data/Somalogic_Analyte_Annotation_All.csv',header=0)\n",
    "    #display(df3.head())\n",
    "    data3 = df3.loc[:, ['SeqId', 'UniProt']]\n",
    "    #display(data3.head())\n",
    "    uniprot_list_v3 = df3['UniProt'].tolist()\n",
    "    sequence_list_v3 = df3['SeqId'].tolist()\n",
    "\n",
    "    df_lod = pd.read_csv(\"../data/Thermo_Fisher_HighProportionProteins_0.2.csv\")\n",
    "    uniprot_lod = df_lod['UniProt'].to_list()\n",
    "\n",
    "\n",
    "    uniprot_to_seqid = dict(zip(data3['UniProt'], data3['SeqId']))\n",
    "\n",
    "    #function to convert uniprot to olinkid\n",
    "    def convert_uniprot_to_seqid(uniprot_list, uniprot_to_seqid):\n",
    "        return list(filter(None, map(lambda uniprot: uniprot_to_seqid.get(uniprot), uniprot_list)))\n",
    "\n",
    "    uniprot_list_set1 = uniprot_list_v1\n",
    "    uniprot_list_set2 = list(set(uniprot_list_v2) - set(uniprot_list_v1))\n",
    "    uniprot_list_set3 = list(set(uniprot_list_v3) - set(uniprot_list_v2))\n",
    "\n",
    "    return uniprot_list_set1, uniprot_list_set2, uniprot_list_set3, uniprot_lod\n",
    "    \n",
    "#filter the data to only select the required proteins\n",
    "\n",
    "def return_set_breakdown (proteinlist):\n",
    "    set1, set2, set3, lod = return_protein_list()\n",
    "    set1_count = len(set(proteinlist) & set(set1))\n",
    "    set2_count = len(set(proteinlist) & set(set2))\n",
    "    set3_count = len(set(proteinlist) & set(set3))\n",
    "    lod_count = len(set(proteinlist) & set(lod))\n",
    "    return set1_count, set2_count, set3_count, lod_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_regression_fdr(df_pheno, df_proteome, Protein_ID, platform_name, df_proteome_median, type):\n",
    "    def run_regression(y, X, family):\n",
    "        X = sm.add_constant(X)  # Add constant term for intercept\n",
    "        if family == 'binomial':\n",
    "            model = sm.GLM(y, X, family=sm.families.Binomial()).fit()\n",
    "        else:\n",
    "            model = sm.GLM(y, X, family=sm.families.Gaussian()).fit()\n",
    "        return model\n",
    "\n",
    "    df_proteome = df_proteome.to_numpy()\n",
    "\n",
    "    # Define the dependent variables and their respective covariates\n",
    "    regression_config = {\n",
    "        'Age': (['Sex', 'Bmi'], 'gaussian'),\n",
    "        'Sex': (['Age', 'Bmi'], 'binomial'),\n",
    "        'Bmi': (['Age', 'Sex'], 'gaussian')\n",
    "    }\n",
    "    # Define common covariates\n",
    "    common_covariates = ['FREG5_Ethnic_Group_I', 'FREG5_Ethnic_Group_M']\n",
    "    results = {dep_var: [] for dep_var in regression_config}\n",
    "\n",
    "    for i in range(df_proteome.shape[1]):\n",
    "        if type=='preanml':\n",
    "            protein_data = df_proteome[:, i]\n",
    "        else:\n",
    "            protein_data = df_proteome[:, i]\n",
    "        \n",
    "        for dep_var, (covariates, family) in regression_config.items():\n",
    "            X = np.column_stack((protein_data, df_pheno[covariates].values, df_pheno[common_covariates].values))\n",
    "            #X = np.column_stack((protein_data, df_pheno[covariates].values))\n",
    "            model = run_regression(df_pheno[dep_var], X, family)\n",
    "            results[dep_var].append([\n",
    "    Protein_ID[i], model.params.iloc[1], model.bse.iloc[1], model.tvalues.iloc[1], model.pvalues.iloc[1]\n",
    "])\n",
    "\n",
    "    significance_value = 0.05\n",
    "    bonferroni_threshold = significance_value / len(Protein_ID)\n",
    "    significant_results = {}\n",
    "    top_10_results = {}\n",
    "    allresults = {}\n",
    "    significant_results_no_corr={}\n",
    "\n",
    "    print(f\"Number of samples: {df_pheno.shape[0]}\")\n",
    "    print(f\"Number of proteins: {df_proteome.shape[1]}\")\n",
    "\n",
    "    for dep_var, result_list in results.items():\n",
    "        columns = ['Protein_ID', 'Est', 'SE', 't_value', 'P']\n",
    "        res_df = pd.DataFrame(result_list, columns=columns)\n",
    "\n",
    "        # Print uncorrected p-value threshold\n",
    "        print(f\"Uncorrected p-value threshold: {significance_value}\")\n",
    "        print(f\"Bonferroni correction threshold: {bonferroni_threshold}\")\n",
    "\n",
    "        res_df = res_df.dropna(subset=['P'])\n",
    "\n",
    "        # FDR correction\n",
    "        res_df['FDR_P'] = multipletests(res_df['P'], method='fdr_bh')[1]\n",
    "\n",
    "        # Bonferroni correction\n",
    "        res_df['Bonferroni_P'] = res_df['P'] < bonferroni_threshold\n",
    "\n",
    "        # Find and print the FDR p-value threshold\n",
    "        fdr_threshold = res_df[res_df['FDR_P'] < significance_value]['FDR_P'].max()\n",
    "        print(f\"FDR-corrected p-value threshold for {dep_var}: {fdr_threshold}\")\n",
    "\n",
    "        print(f\"Number of significant results for {dep_var} (uncorrected): {res_df[res_df['P'] < significance_value].shape[0]}\")\n",
    "        print(f\"Number of significant results for {dep_var} (FDR corrected): {res_df[res_df['FDR_P'] < significance_value].shape[0]}\")\n",
    "        print(f\"Number of significant results for {dep_var} (Bonferroni corrected): {res_df[res_df['Bonferroni_P']].shape[0]}\")\n",
    "\n",
    "        # Filter significant results using FDR-corrected p-values\n",
    "        sig_res_df = res_df[res_df['FDR_P'] < significance_value]\n",
    "        significant_results[dep_var] = sig_res_df\n",
    "\n",
    "        # Annotate with Uniprot ID names\n",
    "        uniprot = pd.read_csv(\"../data/uniprotkb_Human_AND_model_organism_9606_2024_05_20.tsv\", sep='\\t')\n",
    "        uniprot = uniprot.iloc[:, [0, 3, 4, 7]]\n",
    "\n",
    "        final_out = pd.merge(res_df, uniprot, left_on='Protein_ID', right_on='Entry', how='left')\n",
    "        final_out = final_out.sort_values(by='FDR_P')\n",
    "        #final_out = pd.merge(final_out, df_proteome_median, left_on='Protein_ID', right_index=True, how='left')\n",
    "\n",
    "        allresults[dep_var] = final_out\n",
    "        # Save annotated results file\n",
    "        output_file = f\"./output/all/{platform_name}_{dep_var.lower()}_associations_{type}_fdr_corrected.csv\"\n",
    "        final_out.to_csv(output_file, sep='\\t', index=False)\n",
    "        print(f\"Saved all {dep_var} associations to {output_file}\")\n",
    "\n",
    "        #save significant results\n",
    "        sig_output_file = f\"./output/significant/{platform_name}_{dep_var.lower()}_significant_associations_{type}_fdr_corrected.csv\"\n",
    "        sig_res_df.to_csv(sig_output_file, sep='\\t', index=False)\n",
    "        print(f\"Saved significant {dep_var} associations to {sig_output_file}\")\n",
    "\n",
    "        # Store the top 10 significant results for each dependent variable\n",
    "        top_10_results[dep_var] = final_out.head(10)\n",
    "\n",
    "    # Lookup known associations by Uniprot ID (example: P15502)\n",
    "    lookup_results = {dep_var: final_out[final_out['Protein_ID'] == \"P15502\"] for dep_var, final_out in significant_results.items()}\n",
    "\n",
    "    return significant_results, top_10_results, lookup_results, allresults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common proteins (UniProt IDs): 1740\n",
      "Number of corresponding Thermo ProteinIDs: 1779\n",
      "Average Thermo ProteinIDs per UniProt ID: 1.02\n",
      "Max Thermo ProteinIDs for a single UniProt ID: 4\n"
     ]
    }
   ],
   "source": [
    "#generate the column names from the common proteins file\n",
    "common_proteins = pd.read_csv(\"common_proteins_soma_olink_thermo.txt\", header=None)\n",
    "common_proteins.columns = ['Proteins']\n",
    "common_proteins_list = common_proteins['Proteins'].tolist()\n",
    "\n",
    "#convert the common_proteins_list which are uniprot IDs to ALL matching thermo IDs\n",
    "def convert_uniprot_to_thermo(uniprot_list):\n",
    "    thermo_to_uniprot = pd.read_csv(\"Thermo_Fisher_Protein_Annotation_All.csv\")\n",
    "    thermo_to_uniprot = thermo_to_uniprot.loc[:, ['ProteinID', 'UniProt']]\n",
    "    \n",
    "    # Filter rows where UniProt is in the common_proteins_list\n",
    "    matching_proteins = thermo_to_uniprot[thermo_to_uniprot['UniProt'].isin(uniprot_list)]\n",
    "    \n",
    "    # Return all ProteinIDs that match\n",
    "    return matching_proteins['ProteinID'].tolist()\n",
    "\n",
    "common_proteins_thermo = convert_uniprot_to_thermo(common_proteins_list)\n",
    "\n",
    "print(f\"Number of common proteins (UniProt IDs): {len(common_proteins_list)}\")\n",
    "print(f\"Number of corresponding Thermo ProteinIDs: {len(common_proteins_thermo)}\")\n",
    "\n",
    "# Optional: Check the mapping ratio\n",
    "thermo_annotation = pd.read_csv(\"Thermo_Fisher_Protein_Annotation_All.csv\")\n",
    "mapping_counts = thermo_annotation[thermo_annotation['UniProt'].isin(common_proteins_list)].groupby('UniProt').size()\n",
    "print(f\"Average Thermo ProteinIDs per UniProt ID: {mapping_counts.mean():.2f}\")\n",
    "print(f\"Max Thermo ProteinIDs for a single UniProt ID: {mapping_counts.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common(df_unique, use_common_samples=True, use_common_proteins=True):\n",
    "    if use_common_samples:\n",
    "        # Read common samples\n",
    "        common_samples = pd.read_csv(\"common_samples.csv\", header=None)\n",
    "        common_samples.columns = ['FREG0_PID']\n",
    "\n",
    "        # Filter the samples from df_unique\n",
    "        df_unique_filter = df_unique[df_unique['FREG0_PID'].isin(common_samples['FREG0_PID'])]\n",
    "    else:\n",
    "        df_unique_filter = df_unique\n",
    "\n",
    "    if use_common_proteins:\n",
    "        df_unique_proteome = df_unique_filter.iloc[:, 14:]\n",
    "        display(df_unique_proteome.head())\n",
    "        df_unique_proteome = df_unique_proteome[common_proteins_thermo]\n",
    "    else:\n",
    "        df_unique_proteome = df_unique_filter.iloc[:, 14:]\n",
    "\n",
    "    # Separate into phenotype and proteomic datasets\n",
    "    df_pheno_filter = df_unique_filter.iloc[:, :14]\n",
    "    display(df_pheno_filter.head())\n",
    "    df_proteome_filter = df_unique_proteome\n",
    "\n",
    "    # Remove proteins with only one value\n",
    "    df_proteome_filter = df_proteome_filter.loc[:, df_proteome_filter.apply(lambda x: x.nunique()) != 1]\n",
    "\n",
    "    return df_pheno_filter, df_proteome_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function to return the number of unique uniprot IDs given a list of thermo protein IDs\n",
    "def count_unique_uniprot_ids(thermo_protein_ids):\n",
    "    thermo_to_uniprot = pd.read_csv(\"Thermo_Fisher_Protein_Annotation_All.csv\")\n",
    "    thermo_to_uniprot = thermo_to_uniprot.loc[:, ['ProteinID', 'UniProt']]\n",
    "    \n",
    "    # Filter rows where ProteinID is in the thermo_protein_ids\n",
    "    matching_proteins = thermo_to_uniprot[thermo_to_uniprot['ProteinID'].isin(thermo_protein_ids)]\n",
    "    \n",
    "    # Return the number of unique UniProt IDs\n",
    "    return matching_proteins['UniProt'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pheno_common, df_proteome_common = common(df_unique, use_common_samples=True, use_common_proteins=True)\n",
    "#chnage from uniprot \n",
    "Protein_ID_common = df_proteome_common.columns\n",
    "significant_results_common, top_10_common, lookup_result_common, _ = perform_regression_fdr(df_pheno_common, df_proteome_common, Protein_ID_common, \"thermo\", df_proteome_median, type=\"common_protein_common_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: Unique UniProt IDs: 11, Total Thermo IDs: 12\n",
      "Sex: Unique UniProt IDs: 0, Total Thermo IDs: 0\n",
      "Bmi: Unique UniProt IDs: 50, Total Thermo IDs: 51\n"
     ]
    }
   ],
   "source": [
    "# return the number of unique uniprot IDs and total thermo iDS in the significant results common\n",
    "for key in significant_results_common.keys():\n",
    "    unique_uniprot_count = count_unique_uniprot_ids(significant_results_common[key]['Protein_ID'].tolist())\n",
    "    total_thermo_ids = len(significant_results_common[key]['Protein_ID'].tolist())\n",
    "    print(f\"{key}: Unique UniProt IDs: {unique_uniprot_count}, Total Thermo IDs: {total_thermo_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pheno_commonsample, df_proteome_commonsample = common(df_unique, use_common_samples=True, use_common_proteins=False)\n",
    "Protein_ID_commonsample = df_proteome_commonsample.columns\n",
    "\n",
    "significant_results_commonsample, top_10_commonsample, lookup_result_commonsample,_ = perform_regression_fdr(df_pheno_commonsample, df_proteome_commonsample, Protein_ID_commonsample, \"thermo\", df_proteome_median, type=\"all_protein_common_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: Unique UniProt IDs: 14, Total Thermo IDs: 14\n",
      "Sex: Unique UniProt IDs: 0, Total Thermo IDs: 0\n",
      "Bmi: Unique UniProt IDs: 135, Total Thermo IDs: 137\n"
     ]
    }
   ],
   "source": [
    "# count unique uniprot IDs and thermo proteins IDS in the significant results commonsample\n",
    "for key in significant_results_commonsample.keys():\n",
    "    unique_uniprot_count = count_unique_uniprot_ids(significant_results_commonsample[key]['Protein_ID'].tolist())\n",
    "    total_thermo_ids = len(significant_results_commonsample[key]['Protein_ID'].tolist())\n",
    "    print(f\"{key}: Unique UniProt IDs: {unique_uniprot_count}, Total Thermo IDs: {total_thermo_ids}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proteomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
